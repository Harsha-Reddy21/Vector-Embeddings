{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa343818-fa3c-4885-9cb3-ffe88c93fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cb5b37-0885-4875-9736-a82f16652c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae81b0d3-e36a-4764-a1d4-06c8cfc506cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23c4919a-fbe7-488d-8bb3-d743c7cf02d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3462269a-d932-4e92-baf8-87d92b6020ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
    "SERP_API_KEY=os.getenv(\"SERP_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2045ba-a594-4c29-8860-882a52bd40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text() for page in doc])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c973276-a195-43b3-a6fb-64fe4be594a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MCP\\n2025 EDITION\\nTHE ILLUSTRATED\\nGUIDEBOOK \\nAvi Chawla & Akshay Pachaar\\nDailyDoseofDS.com\\nDaily Dose of\\nData Science\\nFREE\\n\\n \\nDailyDoseofDS.com \\nHow to make the most out of \\nthis book and your time? \\nThe reading time of this book is about 3 hours. But not all chapters will be of \\nrelevance to you. This 2-minute assessment will test your current expertise and \\nrecommend chapters that will be most useful to you. \\n \\nScan the QR code below or open this link to start the assessment. It will only take \\n2 minutes to complete. \\n \\n \\nhttps://bit.ly/mcp-assessment \\n \\n1 \\n\\n \\nDailyDoseofDS.com \\nTable of contents \\nSection #1) Model Context Protocol…………….3 \\n1.1) What is MCP?..................................................................................4-5 \\n   Introduction……………………..……………………………………………………………………………4-5 \\n \\n1.2) Why was MCP created?...............................................................6-8 \\n   The problem…………………………………………………………...………………………………………6-7 \\n   The solution……………………………………………………………………………………………………7-8 \\n1.3) MCP Architecture Overview........................................................9-11 \\n   Host…………………………………………………………………………………………………………………….9 \\n   Client…………………………………………………………………………………………………………………10 \\n   Server…………………………………………………………………………………………………………………11 \\n1.4) Tools, Resources and Prompts.................................................12-18 \\n   Tools………………………………………………………………………………………………………………....12 \\n   Resources………………………………………………………………………………………………………….14 \\n   Prompts……………………………………………………………………………………………………………..15 \\nSection #2) MCP Projects……..……………………..19 \\n  2.1) 100% local MCP client………………………………………………………………………………20 \\n  2.2) MCP-powered Agentic RAG…………………………………………………………………….25 \\n  2.3) MCP-powered Financial Analyst…………………………………………………………….29 \\n  2.4) MCP-powered Voice Agent………………………………………………………………………34 \\n  2.5) A uniﬁed MCP server……………………………………………………………………...………..39 \\n  2.6) MCP-powered shared memory for Claude Desktop and Cursor……………43 \\n  2.7) MCP-powered RAG over complex docs…………………………………………………..47 \\n  2.8) MCP-powered Synthetic Data Generator…………………………………..…………..51 \\n  2.9) MCP-powered Deep Researcher…………………...………………………………………….57 \\n  2.10) MCP RAG over videos………………………...…………………………………………………..63 \\n  2.11) MCP-powered Audio Analysis Toolkit…………………………………………………….69 \\n \\n \\n \\n2 \\n\\n \\nDailyDoseofDS.com \\n \\n \\n \\n \\nModel Context \\nProtocol \\n(MCP) \\n \\n3 \\n\\n \\nDailyDoseofDS.com \\nWhat is MCP? \\nImagine you only know English. To get info from a person who only knows: \\n \\n● French, you must learn French. \\n● German, you must learn German. \\n● And so on. \\n \\nIn this setup, learning even 5 languages will be a nightmare for you. \\nBut what if you add a translator that understands all languages? \\n \\n \\n4 \\n\\n \\nDailyDoseofDS.com \\nThis is simple, isn\\'t it? \\n \\nThe translator is like an MCP! \\n \\nIt lets you (Agents) talk to other people (tools or other capabilities) through a \\nsingle interface. \\n \\nTo formalize, while LLMs possess impressive knowledge and reasoning skills, \\nwhich allow them to perform many complex tasks, their knowledge is limited to \\ntheir initial training data. \\n \\n \\nIf they need to access real-time information, they must use external tools and \\nresources on their own. \\n \\nModel context protocol (MCP) is a standardized interface and framework that \\nallows AI models to seamlessly interact with external tools, resources, and \\nenvironments. \\n \\nMCP acts as a universal connector for AI systems to capabilities (tools, etc.), \\nsimilar to how USB-C standardizes connections between electronic devices. \\n5 \\n\\n \\nDailyDoseofDS.com \\nWhy was MCP created? \\nWithout MCP, adding a new tool or integrating a new model was a headache. \\nIf you had three AI applications and three external tools, you might end up \\nwriting nine diﬀerent integration modules (each AI x each tool) because there \\nwas no common standard. This doesn’t scale. \\n \\nDevelopers of AI apps were essentially reinventing the wheel each time, and tool \\nproviders had to support multiple incompatible APIs to reach diﬀerent AI \\nplatforms. \\n \\n \\nLet’s understand this in detail. \\n \\n6 \\n\\n \\nDailyDoseofDS.com \\nThe problem \\nBefore MCP, the landscape of connecting AI to external data and actions looked \\nlike a patchwork of one-oﬀ solutions. \\nEither you hard-coded logic for each tool, managed prompt chains that were not \\nrobust, or you used vendor-speciﬁc plugin frameworks. \\nThis led to the infamous M×N integration problem. \\nEssentially, if you have M diﬀerent AI applications and N diﬀerent tools/data \\nsources, you could end up needing M × N custom integrations. \\nThe diagram below illustrates this complexity: each AI (each “Model”) might \\nrequire unique code to connect to each external service (database, ﬁlesystem, \\ncalculator, etc.), leading to spaghetti-like interconnections. \\n \\nThe solution \\nMCP tackles this by introducing a standard interface in the middle. Instead of M \\n× N direct integrations, we get M + N implementations: each of the M AI \\n7 \\n\\n \\nDailyDoseofDS.com \\napplications implements the MCP client side once, and each of the N tools \\nimplements an MCP server once. \\nNow everyone speaks the same “language”, so to speak, and a new pairing doesn’t \\nrequire custom code since they already understand each other via MCP. \\nThe following diagram illustrates this shift. \\n \\n● On the left (pre-MCP), every model had to wire into every tool. \\n● On the right (with MCP), each model and tool connects to the MCP layer, \\ndrastically simplifying connections. You can also relate this to the \\ntranslator example we discussed earlier.\\n \\n8 \\n\\n \\nDailyDoseofDS.com \\nMCP Architecture Overview \\nAt its heart, MCP follows a client-server architecture (much like the web or other \\nnetwork protocols). \\nHowever, the terminology is tailored to the AI context. There are three main \\nroles to understand: the Host, the Client, and the Server. \\n \\nHost \\nThe Host is the user-facing AI application, the environment where the AI model \\nlives and interacts with the user. \\nThis could be a chat application (like OpenAI’s ChatGPT interface or Anthropic’s \\nClaude desktop app), an AI-enhanced IDE (like Cursor), or any custom app that \\nembeds an AI assistant like Chainlit. \\nHost is the one that initiates connections to the available MCP servers when the \\nsystem needs them. It captures the user\\'s input, keeps the conversation history, \\nand displays the model’s replies. \\n9 \\n\\n \\nDailyDoseofDS.com \\n \\nClient \\nThe MCP Client is a component within the Host that handles the low-level \\ncommunication with an MCP Server. \\nThink of the Client as the adapter or messenger. While the Host decides what to \\ndo, the Client knows how to speak MCP to actually carry out those instructions \\nwith the server. \\n \\n10 \\n\\n \\nDailyDoseofDS.com \\nServer \\nThe MCP Server is the external program or service that actually provides the \\ncapabilities (tools, data, etc.) to the application. \\nAn MCP Server can be thought of as a wrapper around some functionality, which \\nexposes a set of actions or resources in a standardized way so that any MCP \\nClient can invoke them. \\nServers can run locally on the same machine as the Host or remotely on some \\ncloud service since MCP is designed to support both scenarios seamlessly. The \\nkey is that the Server advertises what it can do in a standard format (so the client \\ncan query and understand available tools) and will execute requests coming from \\nthe client, then return results. \\n \\n \\n11 \\n\\n \\nDailyDoseofDS.com \\nTools, Resources and Prompts \\nTools, prompts and resources form the three core capabilities of the MCP \\nframework. Capabilities are essentially the features or functions that the server \\nmakes available.  \\n● Tools: Executable actions or functions that the AI (host/client) can invoke \\n(often with side eﬀects or external API calls). \\n● Resources: Read-only data sources that the AI (host/client) can query for \\ninformation (no side eﬀects, just retrieval). \\n● Prompts: Predeﬁned prompt templates or workﬂows that the server can \\nsupply. \\nTools \\nTools are what they sound like: functions that do something on behalf of the AI \\nmodel. These are typically operations that can have eﬀects or require \\ncomputation beyond the AI’s own capabilities. \\nImportantly, Tools are usually triggered by the AI model’s choice, which means \\nthe LLM (via the host) decides to call a tool when it determines it needs that \\nfunctionality. \\nSuppose we have a simple tool for weather. In an MCP server’s code, it might \\nlook like: \\n12 \\n\\n \\nDailyDoseofDS.com \\n \\nThis Python function, registered with @mcp.tool(), can be invoked by the AI via \\nMCP. \\nWhen the AI calls tools/call with name \"get_weather\" and {\"location\": \"San \\nFrancisco\"} as arguments, the server will execute get_weather(\"San Francisco\") \\nand return the dictionary result. \\nThe client will get that JSON result and make it available to the AI. Notice the \\ntool returns structured data (temperature, conditions), and the AI can then use or \\nverbalize (generate a response) that info. \\nSince tools can do things like ﬁle I/O or network calls, an MCP implementation \\noften requires that the user permit a tool call. \\n13 \\n\\n \\nDailyDoseofDS.com \\n \\nFor example, Claude’s client might pop up “The AI wants to use the ‘get_weather’ \\ntool, allow yes/no?” the ﬁrst time, to avoid abuse. This ensures the human stays in \\ncontrol of powerful actions. \\nTools are analogous to “functions” in classic function calling, but under MCP, \\nthey are used in a more ﬂexible, dynamic context. They are model-controlled but \\ndeveloper/governance-approved in execution. \\nResources  \\nResources provide read-only data to the AI model. \\nThese are like databases or knowledge bases that the AI can query to get \\ninformation, but not modify. \\nUnlike tools, resources typically do not involve heavy computation or side eﬀects, \\nsince they are often just information lookup. \\nAnother key diﬀerence is that resources are usually accessed under the host \\napplication’s control (not spontaneously by the model). In practice, this might \\nmean the Host knows when to fetch a certain context for the model. \\n14 \\n\\n \\nDailyDoseofDS.com \\nFor instance, if a user says, “Use the company handbook to answer my question,” \\nthe Host might call a resource that retrieves relevant handbook sections and \\nfeeds them to the model. \\nResources could include a local ﬁle’s contents, a snippet from a knowledge base \\nor documentation, a database query result (read-only), or any static data like \\nconﬁguration info. \\nEssentially anything the AI might need to know as context. An AI research \\nassistant could have resources like “ArXiv papers database,” where it can retrieve \\nan abstract or reference when asked. \\nA simple resource could be a function to read a ﬁle: \\n \\nHere we use a decorator @mcp.resource(\"ﬁle://{path}\") which might indicate a \\ntemplate for resource URIs. \\nThe AI (or Host) could ask the server for resources.get with a URI like \\nﬁle://home/user/notes.txt, and the server would \\ncallread_ﬁle(\"/home/user/notes.txt\") and return the text. \\nNotice that resources are usually identiﬁed by some identiﬁer (like a URI or \\nname) rather than being free-form functions. \\n15 \\n\\n \\nDailyDoseofDS.com \\nThey are also often application-controlled, meaning the app decides when to \\nretrieve them (to avoid the model just reading everything arbitrarily). \\nFrom a safety standpoint, since resources are read-only, they are less dangerous, \\nbut still, one must consider privacy and permissions (the AI shouldn’t read ﬁles \\nit’s not supposed to). \\nThe Host can regulate which resource URIs it allows the AI to access, or the \\nserver might restrict access to certain data. \\nIn summary, Resources give the AI knowledge without handing over the keys to \\nchange anything. \\nThey’re the MCP equivalent of giving the model reference material when needed, \\nwhich acts like a smarter, on-demand retrieval system integrated through the \\nprotocol. \\nPrompts \\nPrompts in the MCP context are a special concept: they are predeﬁned prompt \\ntemplates or conversation ﬂows that can be injected to guide the AI’s behavior. \\nEssentially, a Prompt capability provides a canned set of instructions or an \\nexample dialogue that can help steer the model for certain tasks. \\nBut why have prompts as a capability? \\nThink of recurring patterns: e.g., a prompt that sets up the system role as “You \\nare a code reviewer,” and the user’s code is inserted for analysis. \\nRather than hardcoding that in the host application, the MCP server can supply \\nit. \\nPrompts can also represent multi-turn workﬂows. \\nFor instance, a prompt might deﬁne how to conduct a step-by-step diagnostic \\ninterview with a user. By exposing this via MCP, any client can retrieve and use \\n16 \\n\\n \\nDailyDoseofDS.com \\nthese sophisticated prompts on demand. \\nAs far as control is concerned, Prompts are usually user-controlled or \\ndeveloper-controlled. \\nThe user might pick a prompt/template from a UI (e.g., “Summarize this \\ndocument” template), which the host then fetches from the server. \\nThe model doesn’t spontaneously decide to use prompts the way it does tools. \\nRather, the prompt sets the stage before the model starts generating. In that \\nsense, prompts are often fetched at the beginning of an interaction or when the \\nuser chooses a speciﬁc “mode”. \\nSuppose we have a prompt template for code review. The MCP server might have: \\n \\nThis prompt function returns a list of message objects (in OpenAI format) that \\nset up a code review scenario. \\nWhen the host invokes this prompt, it gets those messages and can insert the \\nactual code to be reviewed into the user content. \\nThen it provides these messages to the model before the model’s own answer. \\nEssentially, the server is helping to structure the conversation. \\nWhile we have personally not seen much applicability of this yet, common use \\ncases for prompt capabilities include things like “brainstorming guide,” \\n“step-by-step problem solver template,” or domain-speciﬁc system roles. \\n17 \\n\\n \\nDailyDoseofDS.com \\nBy having them on the server, they can be updated or improved without changing \\nthe client app, and diﬀerent servers can oﬀer diﬀerent specialized prompts. \\nAn important point to note here is that prompts, as a capability, blur the line \\nbetween data and instructions. \\nThey represent best practices or predeﬁned strategies for the AI to use. \\nIn a way, MCP prompts are similar to how ChatGPT plugins can suggest how to \\nformat a query, but here it’s standardized and discoverable via the protocol. \\n \\n \\n \\n \\n \\n \\n18 \\n\\n \\nDailyDoseofDS.com \\n \\n \\nMCP Projects \\n \\n19 \\n\\n \\nDailyDoseofDS.com \\n#1) 100% local MCP Client \\nAn MCP client is a component in an AI app (like Cursor) that establishes \\nconnections to external tools. Learn how to build it 100% locally. \\n \\nTech stack: \\n● Llamaindex to build the MCP-powered Agent \\n● Ollama to locally serve Deepseek-R1. \\n● LightningAI for development and hosting \\nWorkﬂow: \\n● User submits a query. \\n● Agent connects to the MCP server to discover tools. \\n● Based on the query, agent invokes the right tool and get context \\n● Agent returns a context-aware response. \\n20 \\n\\n \\nDailyDoseofDS.com \\nLet’s implement this! \\n#1) Build an SQLite MCP Server \\nFor this demo, we\\'ve built a simple SQLite server with two tools: \\n● add data \\n● fetch data \\nThis is done to keep things simple, but the client we\\'re building can connect to \\nany MCP server out there. \\n \\n \\n#2) Set Up LLM \\nWe\\'ll use a locally served Deepseek-R1 via Ollama as the LLM for our \\nMCP-powered agent. \\n \\n21 \\n\\n \\nDailyDoseofDS.com \\n#3) Deﬁne system prompt \\nWe deﬁne our agent’s guiding instructions to use tools before answering user \\nqueries. \\nFeel free to tweak this on a need basis. \\n \\n \\n#4) Deﬁne the Agent \\nWe deﬁne a function that builds a typical LlamaIndex agent with its appropriate \\narguments. \\nThe tools passed to the agent are MCP tools, which llama_index wraps as native \\ntools that can be easily used by our FunctionAgent. \\n \\n22 \\n\\n \\nDailyDoseofDS.com \\n#5) Deﬁne Agent Interaction \\nWe pass user messages to our FunctionAgent with a shared Context for memory, \\nstream tool calls and return its reply. We manage all the chat history and tool \\ncalls here. \\n \\n#6) Initialize MCP Client and the Agent \\nLaunch the MCP client, load its tools, and wrap them as native tools for \\nfunction-calling agents in LlamaIndex. Then, pass these tools to the agents and \\nadd the context manager. \\n \\n23 \\n\\n \\nDailyDoseofDS.com \\n#7) Run the Agent: \\nFinally, we start interacting with our agent and get access to the tools from our \\nSQLite MCP server.  \\n \\n \\n \\n \\nThe code is available here:  \\nhttps://www.dailydoseofds.com/p/bui\\nlding-a-100-local-mcp-client/ \\n \\n \\n24 \\n\\n \\nDailyDoseofDS.com \\n#2) MCP-powered Agentic RAG \\nLearn how to create an MCP-powered Agentic RAG that searches a vector \\ndatabase and falls back to web search if needed. \\n \\nTech stack: \\n● Bright Data to scrape the web at scale. \\n● Qdrant as the vector DB. \\n● Cursor as the MCP client. \\nWorkﬂow: \\n● The user inputs a query through the MCP client (Cursor). \\n● The client contacts the MCP server to select a relevant tool. \\n● The tool output is returned to the client to generate a response. \\nLet’s implement this! \\n#1) Launch an MCP server \\n25 \\n\\n \\nDailyDoseofDS.com \\nFirst, we deﬁne an MCP server with the host URL and port. \\n \\n#2) Vector DB MCP tool \\nA tool exposed through an MCP server has two requirements: \\n● It must be decorated with the \"tool\" decorator. \\n● It must have a clear docstring. \\nBelow, we have an MCP tool to query a vector DB. It stores ML-related FAQs. \\n \\n \\n26 \\n\\n \\nDailyDoseofDS.com \\n#3) Web search MCP tool \\nIf query is unrelated to ML, we resort to web search using Bright Data\\'s SERP \\nAPI to scrape data at scale across several sources to get relevant context. \\n \\n \\n#4) Integrate MCP server with Cursor \\nGo to Settings → MCP → Add new global MCP server.  In the JSON ﬁle, add \\nwhat\\'s shown below \\n \\n27 \\n\\n \\nDailyDoseofDS.com \\nDone!  \\nYour local MCP server is live and connected to Cursor. It has two MCP tools: \\n● Bright Data web search tool to scrape data at scale. \\n● Vector DB search tool to query the relevant documents. \\n \\nNext, we interact with the MCP server. \\n● When we ask an ML-related query, it invokes the vector DB tool. \\n● But when we ask a general query, it invokes the Bright Data web search \\ntool to gather web data at scale from various sources. \\nThat\\'s Agentic behavior! \\n \\n \\nThe code is available here: \\nhttps://www.dailydoseofds.com/p/mc\\np-powered-agentic-rag/ \\n  \\n \\n28 \\n\\n \\nDailyDoseofDS.com \\n#3) MCP-powered Financial Analyst \\nBuild an MCP-powered AI agent that fetches, analyzes & generates insights on \\nstock market trends, right from Cursor or Claude Desktop. \\n \\nTech stack: \\n● CrewAI for multi-agent orchestration \\n● Ollama to locally serve DeepSeek-R1 LLM \\n● Cursor as the MCP host \\nWorkﬂow: \\n● User submits a query. \\n● The MCP agent kicks oﬀ the ﬁnancial analyst crew. \\n● The crew conducts research and creates an executable script. \\n● The agent runs the script to generate an analysis plot. \\n29 \\n\\n \\nDailyDoseofDS.com \\n#1) Setup LLM \\nWe will use Deepseek-R1 as the LLM, served locally using Ollama. \\n \\nLet\\'s setup the Crew now \\n#2) Query Parser Agent \\nThis agent accepts a natural language query and extracts structured output using \\nPydantic. This guarantees clean and structured inputs for further processing! \\n \\n30 \\n\\n \\nDailyDoseofDS.com \\n#3) Code Writer Agent \\nThis agent writes Python code to visualize stock data using Pandas, Matplotlib, \\nand Yahoo Finance libraries. \\n \\n#4) Code Executor Agent \\nThis agent reviews and executes the generated Python code for stock data \\nvisualization. \\nIt uses the code interpreter tool by CrewAI to execute the code in a secure \\nsandbox environment. \\n \\n31 \\n\\n \\nDailyDoseofDS.com \\n#5) Setup Crew and Kickoﬀ \\nWe set up and kick oﬀ our ﬁnancial analysis crew to get the result shown below! \\n \\n#6) Create MCP Server \\nNow, we encapsulate our ﬁnancial analyst within an MCP tool and add two more \\ntools to enhance the user experience. \\n● save_code -> Saves generated code to local directory \\n● run_code_and_show_plot -> Executes the code and generates a plot \\n \\n32 \\n\\n \\nDailyDoseofDS.com \\n#7) Integrate MCP server with Cursor \\nGo to: File → Preferences → Cursor Settings → MCP → Add new global MCP \\nserver. In the JSON ﬁle, add what\\'s shown below \\n \\nDone! Our ﬁnancial analyst MCP server is live and connected to Cursor. \\n \\n \\n \\nThe code is available here: \\nhttps://www.dailydoseofds.com/p/hands-o\\nn-building-an-mcp-powered-ﬁnancial-an\\nalyst/ \\n \\n33 \\n\\n \\nDailyDoseofDS.com \\n \\n#4) MCP-powered Voice Agent \\nThis project teaches you how to build an MCP-driven voice Agent that queries a \\ndatabase and falls back to web search if needed. \\n \\n \\nTech Stack \\n● AssemblyAI for Speech‐to‐Text. \\n● Firecrawl for web search. \\n● Supabase for a database. \\n● Livekit for orchestration. \\n● Qwen3 as the LLM. \\nWorkﬂow: \\n● User\\'s speech query is transcribed to text with AssemblyAI. \\n● Agent discovers DB & web tools. \\n● LLM invokes the right tool, fetches data & generates a response. \\n● The app delivers the response via text-to-speech. \\n34 \\n\\n \\nDailyDoseofDS.com \\nLet’s implement this! \\n#1) Initialize Firecrawl & Supabase \\nWe instantiate Firecrawl to enable web searches and start our MCP server to \\nexpose Supabase tools to our Agent. \\n \\n#2) Deﬁne web search tool \\nWe fetch live web search results using Firecrawl search endpoint. This gives our \\nagent up-to-date online information. \\n35 \\n\\n \\nDailyDoseofDS.com \\n \\n#3) Get Supabase MCP Tools \\nWe list our Supabase tools via the MCP server and wrap each of them as LiveKit \\ntools for our Agent. \\n \\n36 \\n\\n \\nDailyDoseofDS.com \\n#4) Build the Agent \\nWe set up our Agent with instructions on how to handle user queries. We also \\ngive it access to the Firecrawl web search and Supabase tools deﬁned earlier. \\n \\n#5) Conﬁgure Speech-to-Response ﬂow \\n● We transcribe user speech with AssemblyAI Speech-to-Text. \\n● Qwen 3 LLM, served locally with Ollama, invokes the right tool. \\n● A voice output is generated via TTS. \\n \\n37 \\n\\n \\nDailyDoseofDS.com \\n#6) Launch the Agent \\nWe connect to LiveKit and start our session with a greeting. Then continuously \\nlisten and respond until the user stops. \\n \\nDone!  \\nOur MCP-powered Voice Agent is ready. \\n● If the query is related to a database, it queries Supabase via MCP tools. \\n● Otherwise, it performs a web search via Firecrawl. \\n \\n \\n \\n \\nThe code is available here: \\nhttps://www.dailydoseofds.com/p/an\\n-mcp-powered-voice-agent/ \\n \\n38 \\n\\n \\nDailyDoseofDS.com \\n \\n39 \\n\\n \\nDailyDoseofDS.com \\n#5) A Uniﬁed MCP server \\nThis project builds an MCP server to query and chat with over 200+ data sources \\nusing natural language through a uniﬁed interface powered by MindsDB and \\nCursor IDE. \\n \\nTech stack \\n● MindsDB to power our uniﬁed MCP server \\n● Cursor as the MCP host \\n● Docker to self-host the server \\nWorkﬂow \\n● User submits a query \\n● Agent connects to the MindsDB MCP server to ﬁnd tools \\n● Selects the appropriate tool based on the user query and calls it \\n● Finally, returns a contextually relevant response \\nLet’s implement this! \\n40 \\n\\n \\nDailyDoseofDS.com \\n#1) Docker Setup \\nMindsDB provides Docker images that can be run in Docker containers. \\nInstall MindsDB locally using the Docker image by running the command in your \\nterminal. \\n \\n#2) Start MindsDB GUI \\nAfter installing the Docker image, go to 127.0.0.1:47334 in your browser to access \\nthe MindsDB editor. \\nThrough this interface, you can connect to over 200 data sources and run SQL \\nqueries against them. \\n#3) Integrate Data Sources \\nLet\\'s start building our federated query engine by connecting our data sources to \\nMindsDB. \\nWe use Slack, Gmail, GitHub and Hacker News as our federated data sources. \\n41 \\n\\n \\nDailyDoseofDS.com \\n \\n#4) Integrate MCP Server with Cursor \\nAfter building the federated query engine, let\\'s unify our data sources by \\nconnecting them to MindsDB\\'s MCP server. \\nGo to: File → Preferences → Cursor Settings → MCP → Add new global MCP \\nserver. In the JSON ﬁle, add the following  \\n \\n42 \\n\\n \\nDailyDoseofDS.com \\nDone! Our MindsDB MCP server is live and connected to Cursor! \\nThe MCP server oﬀers two tools: \\n● list_databases: Lists all data sources connected to MindsDB. \\n● query: Answers user queries on the federated data. \\n \\nApart from Claude and Cursor, MindsDB MCP server also works with the new \\nOpenAI MCP integration. \\n \\n \\n \\nThe code is available here: \\nhttps://www.dailydoseofds.com/p/buil\\nd-an-mcp-server-to-connect-to-200-d\\nata-sources/ \\n43 \\n\\n \\nDailyDoseofDS.com \\n#6) MCP-powered shared memory for Claude \\nDesktop and Cursor \\nDevs use Claude Desktop and Cursor independently with no context sharing. \\nLearn how to add a common memory layer to cross-operate without losing \\ncontext. \\n \\nTech Stack \\n● Zep’s Graphiti MCP as a memory layer for AI Agents. \\n● Cursor and Claude as the MCP hosts. \\nWorkﬂow \\n● User submits a query to Cursor & Claude. \\n● Facts/Info are stored in a common memory layer using Graphiti MCP. \\n● Memory is queried if context is required in any interaction. \\n● Graphiti shares memory across multiple hosts. \\n44 \\n\\n \\nDailyDoseofDS.com \\n#1) Docker Setup \\nDeploy the Graphiti MCP server using Docker Compose. This setup starts the \\nMCP server with Server-Sent Events (SSE) transport. \\n \\nThe Docker setup above includes a Neo4j container, which launches the database \\nas a local instance. \\nThis conﬁguration lets you query and visualize the knowledge graph using the \\nNeo4j browser preview. \\n \\n45 \\n\\n \\nDailyDoseofDS.com \\n#2) Connect MCP server to Cursor \\nWith tools and our server ready, let\\'s integrate it with our Cursor IDE! \\nGo to: File → Preferences → Cursor Settings → MCP → Add new global MCP \\nserver. In the JSON ﬁle, add what\\'s shown below \\n \\n#3) Connect MCP server with Claude \\nGo to File → Settings → Developer → Edit Conﬁg, add what\\'s shown below \\n \\nDone! \\n46 \\n\\n \\nDailyDoseofDS.com \\nOur Graphiti MCP server is live and connected to Cursor & Claude! \\n \\nNow you can chat with Claude Desktop, share facts/info, store the response in \\nmemory, and retrieve them from Cursor, and vice versa. \\nThis way, you can pipe Claude’s insights straight into Cursor, all via a single \\nMCP. \\n \\n \\n \\nThe code is available here: \\nhttps://www.dailydoseofds.com/p/build-a\\n-shared-memory-for-claude-desktop-and\\n-cursor/ \\n \\n47 \\n\\n \\nDailyDoseofDS.com \\n#7) MCP-powered RAG over complex docs \\nLearn how to use MCP to power an RAG app over complex documents with \\ntables, charts, images, complex layouts, and whatnot. \\n \\nTech Stack \\n● Cursor as the MCP client \\n● EyelevelAI\\'s GroundX to build an MCP server that can process complex \\ndocs \\nWorkﬂow \\n● User interacts with the MCP client (Cursor IDE) \\n● Client connects to the MCP server and selects a tool. \\n● Tools leverage GroundX to do an advanced search over docs \\n● Search results are used by Client to generate response \\n48 \\n\\n \\nDailyDoseofDS.com \\nLet’s implement this! \\n#1) Setup server \\nFirst we setup a local MCP server, using FastMCP and provide it a name \\n \\n#2) Create GroundX Client \\nGroundX oﬀers capabilities document search and retrieval capabilities for \\ncomplex real-world documents. \\nHere\\'s how to set up a client: \\n \\n \\n49 \\n\\n \\nDailyDoseofDS.com \\n#3) Create Ingestion tool \\nThis tool is used to ingest new documents into the knowledge base. User just \\nneeds to provide a path to the document to be ingested: \\n \\n#4) Create Search tool \\nThis tool leverages GroundX\\'s advanced capabilities to do search and retrieval \\nfrom complex real world documents. Here\\'s how to implement it: \\n \\n50 \\n\\n \\nDailyDoseofDS.com \\n#5) Start the server \\nStarts an MCP server using stdio as the transport mechanism: \\n \\n#6) Connect to Cursor  \\nInside you Cursor IDE follow this: Cursor → Settings → Cursor Settings → MCP \\nThen add and start your server like this: \\n \\n \\n \\nThe code is available here: \\nhttps://www.dailydoseofds.com/p/mcp-\\npowered-rag-over-complex-docs/ \\n \\n51 \\n\\n \\nDailyDoseofDS.com \\n#8) MCP-powered synthetic data generator \\nLearn how to build an MCP server that can generate any type of synthetic \\ndataset. It uses Cursor as the MCP host and SDV to generate realistic tabular \\nsynthetic data. \\n \\nTech Stack \\n● Cursor as the MCP host \\n● Datacebo\\'s SDV to generate realistic tabular synthetic data \\nWorkﬂow \\n● User submits a query \\n● Agent connects to MCP server to ﬁnd tools \\n● Agent uses appropriate tool based on query \\n● Returns response on synthetic data creation, eval, or visualization \\n52 \\n\\n \\nDailyDoseofDS.com \\nHere’s an overview of our MCP server, which includes three tools: \\n● SDV Generate \\n● SDV Evaluate \\n● SDV Visualise \\nWe have kept the actual implementation of these tools using the SDV SDK in a \\nseparate ﬁle, tools[.]py, that is imported here. \\n \\nNow let\\'s look at each tool in more details. \\n53 \\n\\n \\nDailyDoseofDS.com \\n#1) SDV Generate Tool \\nThis tool creates synthetic data from real data using the SDV Synthesizer. \\nSDV oﬀers a variety of synthesizers, each utilizing diﬀerent algorithms to \\nproduce synthetic data. \\n \\n#2) SDV Evaluate Tool \\nThis tool evaluates the quality of synthetic data in comparison to real data. \\nWe will assess statistical similarity to determine which real data patterns are \\ncaptured by the synthetic data. \\n54 \\n\\n \\nDailyDoseofDS.com \\n \\n#3) SDV Visualize Tool \\nThis tool generates a visualization to compare real and synthetic data for a \\nspeciﬁc column. \\nUse this function to visualize a real column alongside its corresponding synthetic \\ncolumn. \\n55 \\n\\n \\nDailyDoseofDS.com \\n \\nWith tools and server ready, lets integrate it with our Cursor IDE! Go to: File → \\nPreferences → Cursor Settings → MCP → Add new global MCP server.  In the \\nJSON ﬁle, add what\\'s shown below \\n \\nDone! Your synthetic data generator MCP server is live and connected to Cursor. \\n56 \\n\\n \\nDailyDoseofDS.com \\n \\n \\n \\nThe code is available here: \\nhttps://www.dailydoseofds.com/p/hands-on\\n-mcp-powered-synthetic-data-generator/ \\n \\n57 \\n\\n \\nDailyDoseofDS.com \\n#9) MCP-powered deep researcher \\nChatGPT has a deep research feature. It helps you get detailed insights on any \\ntopic. Learn how you can build a 100% local alternative to it. \\n \\nTech Stack \\n● Linkup platform for deep web research \\n● CrewAI for multi-agent orchestration \\n● Ollama to locally serve DeepSeek \\n● Cursor as MCP host \\nWorkﬂow \\n● User submits a query \\n● Web search agent runs deep web search via Linkup \\n● Research analyst veriﬁes and deduplicates results \\n● Technical writer crafts a coherent response with citations \\n58 \\n\\n \\nDailyDoseofDS.com \\n#1) Setup LLM \\nWe\\'ll use a locally served DeepSeek-R1 using Ollama. \\n  \\n#2) Deﬁne Web Search Tool \\nWe\\'ll use Linkup platform\\'s powerful search capabilities, which rival Perplexity \\nand OpenAI, to power our web search agent. This is done by deﬁning a custom \\ntool that our agent can use. \\n \\n59 \\n\\n \\nDailyDoseofDS.com \\n#3) Deﬁne Web Search Agent \\nThe web search agent gathers up-to-date information from the internet based on \\nuser query. The linkup tool we deﬁned earlier is used by this agent. \\n \\n#4) Deﬁne Research Analyst Agent \\nThis agent transforms raw web search results into structured insights, with \\nsource URLs. It can also delegate tasks back to the web search agent for \\nveriﬁcation and fact-checking. \\n \\n60 \\n\\n \\nDailyDoseofDS.com \\n#5) Deﬁne Technical Writer Agent \\nIt takes the analyzed and veriﬁed results from the analyst agent and drafts a \\ncoherent response with citations for the end user. \\n \\n#6) Setup Crew \\nFinally, once we have all the agents and tools deﬁned we set up and kickoﬀ our \\ndeep researcher crew. \\n \\n61 \\n\\n \\nDailyDoseofDS.com \\n#7) Create MCP Server \\nNow, we\\'ll encapsulate our deep research team within an MCP tool. With just a \\nfew lines of code, our MCP server will be ready. \\nLet\\'s see how to connect it with Cursor. \\n \\n#8) Integrate MCP server with Cursor \\nGo to: File → Preferences → Cursor Settings → MCP → Add new global MCP \\nserver \\nIn the JSON ﬁle, add what\\'s shown below  \\n62 \\n\\n \\nDailyDoseofDS.com \\n \\nDone! Your deep research MCP server is live and connected to Cursor. \\n \\n \\n \\nThe code is available here: \\nhttps://www.dailydoseofds.com/p/hands-\\non-mcp-powered-deep-researcher/ \\n63 \\n\\n \\nDailyDoseofDS.com \\n#10) MCP-powered RAG over videos \\nWe have an MCP-driven video RAG that ingests a video and lets you chat with it. \\nIt also fetches the exact video chunk where an event occurred. \\n \\nTech Stack \\n● RagieAI for video ingestion and retrieval. \\n● Cursor as the MCP host. \\nWorkﬂow \\n● User speciﬁes video ﬁles and a query. \\n● An Ingestion tool indexes the videos in Ragie. \\n● A Query tool retrieves info from Ragie Index with citations. \\n● Show-video tool returns the video chunk that answers the query \\n64 \\n\\n \\nDailyDoseofDS.com \\nLet’s implement this! \\n#1) Ingest data \\nWe implement a method to ingest video ﬁles into the Ragie index. \\nWe also specify the audio-video mode to load both audio and video channels \\nduring ingestion. \\n \\n#2) Retrieve data \\nWe retrieve the relevant chunks from the video based on the user query. \\n65 \\n\\n \\nDailyDoseofDS.com \\nEach chunk has a start time, an end time, and a few more details that correspond \\nto the video segment. \\n \\n#3) Create MCP Server \\nWe integrate our RAG pipeline into an MCP server with 3 tools: \\n● ingest_data_tool: Ingests data into Ragie index \\n● retrieve_data_tool: Retrieves data based on the user query \\n● show_video_tool: Creates video chunks from the original video \\n66 \\n\\n \\nDailyDoseofDS.com \\n \\n \\n#4) Integrate MCP server with Cursor \\nTo integrate the MCP server with Cursor, go to Settings → MCP → Add new \\nglobal MCP server. \\n67 \\n\\n \\nDailyDoseofDS.com \\n \\nDone!  \\nYour local Ragie MCP server is live and connected to Cursor! \\n \\n68 \\n\\n \\nDailyDoseofDS.com \\nNext, we interact with the MCP server through Cursor. \\nBased on the query, it can: \\n● Ingest a new video into the Ragie Index. \\n● Fetch detailed information about an existing video. \\n● Retrieve the video segment where a speciﬁc event occurred. \\nAnd that was your MCP-powered video RAG. \\n \\n \\n \\n \\nThe code is available here: \\nhttps://www.dailydoseofds.com/p/build-\\nan-mcp-powered-rag-over-videos/ \\n \\n \\n \\n69 \\n\\n \\nDailyDoseofDS.com \\n#11) MCP-powered Audio Analysis Toolkit \\nWe have an MCP-driven audio analysis toolkit that accepts an audio ﬁle and lets \\nyou transcribe it and extract insights such as sentiment analysis, speaker labels, \\nsummary and topic detection. It also lets you chat with audio. \\n \\nTech stack \\n● AssemblyAI for transcription and audio analysis. \\n● Claude Desktop as the MCP host. \\n● Streamlit for the UI \\nWorkﬂow \\n● User\\'s audio input is sent to AssemblyAI via a local MCP server. \\n● AssemblyAI transcribes it while providing the summary, speaker labels, \\nsentiment, and topics. \\n● Post-transcription, the user can also chat with audio. \\n70 \\n\\n \\nDailyDoseofDS.com \\n#1) Transcription MCP tool \\nThis tool accepts an audio input from the user and transcribes it using \\nAssemblyAI. We also store the full transcript to use in the next tool. \\n \\n#2) Audio analysis tool \\nNext, we have a tool that returns speciﬁc insights from the transcript, like \\nspeaker labels, sentiment, topics, and summary. \\n \\n71 \\n\\n \\nDailyDoseofDS.com \\n#3) Create MCP Server \\nNow, we’ll set up an MCP server to use the tools we created above. \\n \\n#4) Integrate MCP server with Claude Desktop \\nGo to File → Settings → Developer → Edit Conﬁg and add the following code. \\n \\n72 \\n\\n \\nDailyDoseofDS.com \\nOnce the server is conﬁgured, Claude Desktop will show the two tools we built \\nabove in the tools menu: \\n● transcribe_audio \\n● get_audio_data \\n \\nAnd that was our MCP-powered audio analysis toolkit! \\nFor accessibility, we have created a Streamlit UI for the audio analysis app. \\nYou can upload the audio, extract insights, and chat with it using AssemblyAI’s \\nLeMUR. Find the code below. \\n \\n \\nThe code is available here: \\nhttps://www.dailydoseofds.com/p/hands-o\\nn-build-an-mcp-powered-audio-analysis-\\ntoolkit/ \\n \\n73 \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_pdf=r\"C:\\Users\\harsh\\Downloads\\MCP.pdf\"\n",
    "text=extract_text_from_pdf(sample_pdf)\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88507268-ce30-492d-b505-fbafe5861776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Text Chunking\n",
    "def chunk_text(text, max_words=100):\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa9dd4c-bbff-4e50-9860-ab99225f6b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MCP 2025 EDITION THE ILLUSTRATED GUIDEBOOK Avi Chawla & Akshay Pachaar DailyDoseofDS.com Daily Dose of Data Science FREE DailyDoseofDS.com How to make the most out of this book and your time? The reading time of this book is about 3 hours. But not all chapters will be of relevance to you. This 2-minute assessment will test your current expertise and recommend chapters that will be most useful to you. Scan the QR code below or open this link to start the assessment. It will only take 2 minutes to complete. https://bit.ly/mcp-assessment 1 DailyDoseofDS.com Table of contents Section #1) Model Context'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=chunk_text(text)\n",
    "len(chunks)\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33865b46-78a5-44d0-9c7d-96b8a74f33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Embed and Store in Vector DB\n",
    "def store_chunks(chunks):\n",
    "    client = chromadb.Client()\n",
    "    collection = client.create_collection(\"pdf_chunks\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        emb = embedding_model.encode(chunk)\n",
    "        collection.add(documents=[chunk], ids=[f\"chunk_{i}\"], embeddings=[emb.tolist()])\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a9c244d-68e1-47ef-91d3-18c9481733fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "SERP_API_KEY = os.getenv(\"SERP_API_KEY\")  # Or hardcode it for testing\n",
    "\n",
    "def web_search_serper(query):\n",
    "    if not SERP_API_KEY:\n",
    "        raise ValueError(\"❌ SERP_API_KEY is missing or not set.\")\n",
    "    \n",
    "    headers = {\n",
    "        \"X-API-KEY\": SERP_API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\"q\": query, \"num\": 5}\n",
    "    \n",
    "    res = requests.post(\"https://google.serper.dev/search\", headers=headers, json=payload)\n",
    "    print(res.status_code)\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "        return res.json().get(\"organic\", [])\n",
    "    else:\n",
    "        print(\"❌ Error:\", res.text)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a762060-f533-4cf1-9ce4-851c6231c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Sparse Retrieval (TF-IDF)\n",
    "def sparse_search(query, chunks):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(chunks)\n",
    "    q_vec = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(q_vec, X).flatten()\n",
    "    ranked = sorted(zip(chunks, scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8806dde-9354-45fe-aeca-2728639eeab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"as the transport mechanism: #6) Connect to Cursor Inside you Cursor IDE follow this: Cursor → Settings → Cursor Settings → MCP Then add and start your server like this: The code is available here: https://www.dailydoseofds.com/p/mcp- powered-rag-over-complex-docs/ 51 DailyDoseofDS.com #8) MCP-powered synthetic data generator Learn how to build an MCP server that can generate any type of synthetic dataset. It uses Cursor as the MCP host and SDV to generate realistic tabular synthetic data. Tech Stack ● Cursor as the MCP host ● Datacebo's SDV to generate realistic tabular synthetic data Workﬂow ● User submits a query ● Agent connects\",\n",
       "  0.16040518476246288),\n",
       " (\"MCP? Imagine you only know English. To get info from a person who only knows: ● French, you must learn French. ● German, you must learn German. ● And so on. In this setup, learning even 5 languages will be a nightmare for you. But what if you add a translator that understands all languages? 4 DailyDoseofDS.com This is simple, isn't it? The translator is like an MCP! It lets you (Agents) talk to other people (tools or other capabilities) through a single interface. To formalize, while LLMs possess impressive knowledge and reasoning skills, which allow them to perform many\",\n",
       "  0.1489366349389499),\n",
       " ('MCP 2025 EDITION THE ILLUSTRATED GUIDEBOOK Avi Chawla & Akshay Pachaar DailyDoseofDS.com Daily Dose of Data Science FREE DailyDoseofDS.com How to make the most out of this book and your time? The reading time of this book is about 3 hours. But not all chapters will be of relevance to you. This 2-minute assessment will test your current expertise and recommend chapters that will be most useful to you. Scan the QR code below or open this link to start the assessment. It will only take 2 minutes to complete. https://bit.ly/mcp-assessment 1 DailyDoseofDS.com Table of contents Section #1) Model Context',\n",
       "  0.13247699799419327)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='what type of data you have?'\n",
    "res=sparse_search(query,chunks)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c00d1b-fc1c-4147-bead-2000b15b8e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b4ef4-ff85-4187-a78e-7b36a38b5d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6df9f6d-d6cc-4351-96db-9549ebd90096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Hybrid Retrieval (Dense + Sparse Fusion)\n",
    "def hybrid_retrieval(query, chunks, collection):\n",
    "    dense_emb = embedding_model.encode(query)\n",
    "    dense_result = collection.query(query_embeddings=[dense_emb.tolist()], n_results=3)\n",
    "    dense_chunks = dense_result['documents'][0]\n",
    "    dense_scores = [0.7] * len(dense_chunks)  # mock scores\n",
    "\n",
    "    sparse_chunks = sparse_search(query, chunks)\n",
    "    sparse_texts = [s[0] for s in sparse_chunks]\n",
    "    sparse_scores = [0.3] * len(sparse_texts)  # mock scores\n",
    "\n",
    "    combined = list(zip(dense_chunks, dense_scores)) + list(zip(sparse_texts, sparse_scores))\n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [c[0] for c in combined[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87680153-1e60-466d-8436-5ea08c6d3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Generate Answer with Groq\n",
    "def generate_response(query, pdf_sources, web_sources):\n",
    "    context = \"\\n\\n\".join([f\"[PDF] {s}\" for s in pdf_sources] + [f\"[Web] {s['snippet']} (URL: {s['link']})\" for s in web_sources])\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful research assistant. Answer using sources provided.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Query: {query}\\n\\nSources:\\n{context}\\n\\nAnswer using the most relevant and trustworthy information. Cite source type and link or PDF.\"}\n",
    "    ]\n",
    "    res = requests.post(\n",
    "        url=\"https://api.groq.com/openai/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"model\": \"llama3-70b-8192\",\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.3\n",
    "        }\n",
    "    )\n",
    "    return res.json()['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e27df-a226-4f37-a3a2-372cd2d0dc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8dab86-ed58-4a2b-b92e-9165e172fc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "713bb47c-e209-414d-8097-500306b08d53",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Execution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mExecution\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_pipeline\u001b[39m(pdf_path, query):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExtracting PDF...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Execution' is not defined"
     ]
    }
   ],
   "source": [
    "Execution\n",
    "def run_pipeline(pdf_path, query):\n",
    "    print(\"\\nExtracting PDF...\")\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(pdf_text)\n",
    "\n",
    "    print(\"\\nStoring Chunks in Vector DB...\")\n",
    "    collection = store_chunks(chunks)\n",
    "\n",
    "    print(\"\\nRunning Hybrid Retrieval...\")\n",
    "    top_pdf_chunks = hybrid_retrieval(query, chunks, collection)\n",
    "\n",
    "    print(\"\\nSearching Web...\")\n",
    "    top_web_results = web_search_serper(query)\n",
    "\n",
    "    print(\"\\nGenerating Answer...\")\n",
    "    response = generate_response(query, top_pdf_chunks, top_web_results)\n",
    "    print(\"\\nFinal Answer:\\n\")\n",
    "    print(response)\n",
    "\n",
    "\n",
    "run_pipeline(sample_pdf, \"What are recent methods for fine-tuning GPT models?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb39f42-d4a3-40ad-8c4d-ef1ffd2513ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
